---
title: "2022 Feb"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
```

## Problem 1  

```{r, echo=T, eval=F}
 perform a sample size calculation with an 80% power, on the basis of reported clinical and radiographical failure rates of pulpotomy treatment which ranges between 10%-20% (for each) in the literature. `
```



## Problem 2 

```{r, echo=T, eval=F}
Will it it be possible if we make estimatations of sample sizes for 10%, 12% and 15% differences (respectively) between the groups with significance level of 0.05? 
```


## Problem formulation 

The observed sample is summarized in the Table 

|group  | success | failure | total | setting 1 | setting 2 | setting 3 |
|-------|---------|---------|-------|-----------|-----------|-----------|-
|1      | 32      | 2       | 34    | 10%   | 17%       | 25%           |
|2      | 32      |  1      | 33    | 10%   | 17%       | 25%           |
|3      | 34      | 0       | 34    | 0%    | 5%        | 10%           |
|total  | 98      | 3       | 101


Question: With different failure rates under each setting, what is the sample size required to achieve 80% power? 

### three group simultaneous test 

3 different scenarios, the error rate in group 1, 2 are 10%, 17% and 25% and error rate in group 3 are 0%, 5% and 10%, resulting in the error difference 10%, 12%, 15%. Let $p_i$ be the failure rate, $i=1, 2, 3$. Consider the test $H_0:p_1=p_2=p_3$, $H_a:not ~all~ of~ them ~are ~equal$, Use chi square test $$X^2=\sum \frac{(n_{ij}-\hat{\mu}_{ij})^2}{\hat{\mu}_{ij}}$$,   

where $n_{ij}$ are observed counts and $\hat{\mu}_{ij}$ are expected counts under $H_0$. $H_0$
 is rejected when $X>X_{\alpha}(2)$
 
```{r, echo=T}
n1=34; n2=33; n3=34
fail1=2; fail2=1; fail3=0
p_est=3/(n1+n2+n3)
nij=c(32,2,32,1,34,0)
muij=c(n1*(1-p_est), n1*p_est, n2*(1-p_est), n2*p_est, n3*(1-p_est), n3*p_est)
test_stat=sum((nij-muij)^2/muij)
test_stat  # critical value is 5.99 at 5% with 2 DF. 
1-pchisq(test_stat, df=2, ncp = 0, lower.tail = TRUE, log.p = FALSE)
```
Since all sample size are nearly identical, could use balanced study design.

#### power.anova.test

see [this example](https://stats.oarc.ucla.edu/r/dae/one-way-anova-power-analysis/) 

```{r, echo=T}
overall_var=p_est*(1-p_est)*(n1+n2+n3)
########## the key is to set within.var
#group_means=c(0/n1, 1/n2, 2/n3)
group_means=c(0.17,0.17, 0.05 )
#group_means=c(0.17, 0.17, 0.05)
#within.var=0.03 seems reasonable 
p=power.anova.test(groups = length(group_means),
between.var = var(group_means), within.var=0.9, # use variance in group 1 
power=0.8,sig.level=0.05,n=NULL)
p
```



#### pwr.annova.test


use function `pwr.annova.test`
[the example](https://cran.r-project.org/web/packages/pwr/vignettes/pwr-vignette.html)


```{r, echo=T}
library(pwr)
pwr.anova.test(k = 3, f = 5/3, sig.level = 0.05, power = 0.8)
```


```{r, echo=T}
effect_size=seq(0.1,1, by=0.1); sample_size=numeric()
for (i in 1:length(effect_size))
{
  sample_size_calculation=pwr.anova.test(k=3, f=effect_size[i], sig.level=0.05, power=0.8)
  sample_size[i]=sample_size_calculation$n
  
}

ggplot(data=data.frame(effect_size=effect_size, sample_size=round(sample_size)), aes(x=effect_size, y=sample_size, group=1)) +
  geom_line(color="red")+
  #geom_hline(yintercept=0.8, linetype="dashed", color = "blue")+
  geom_point()+  
  geom_hline(yintercept=32, linetype="dashed", color = "blue")
#sample_size=pwr.anova.test(k=3, 0, sig.level=0.05, power=0.8)
```

where $f$ is effect size calculated as $\frac{\sigma_{means}}{\sigma_{pop/n}}$ã€‚ where $\sigma_{means}$
 is the standard deviation of the k means and $\sigma_{pop/n}$
 is the common standard deviation of the k groups. These two quantities are also known as the between-group and within-group standard deviations. 

```{r, echo=T}
sigma_means=sd(c(0.1, 0.1, 0))
sigma_pop=(101*(3/101)*(1-3/101))^0.5
f=sigma_means/sigma_pop
pwr.anova.test(k=3, f=0.3, sig.level=0.05, power=0.8)

```




### one sample test 

#### power vs sample size 

Because sample size is not big (<40), it's better to assume binomial distribution for number of failures, $X$. Consider the hypothesis test, $H_0: p=p_0$ vs $H_a: p< p_0$. The power is defined as $P(reject~ H_0|H_a)=P(X<=X_0|p)=\sum_{X=0}^{X_0} {n \choose k}p^k(1-p)^{n-k}$, so power is a function of following three parameters 

* $p$ is success probability in alternative hypothesis 
* $n$  is sample size 
* $X_0$ is rejection cutoff such that X is rejected when $X<X_0$, at specified significance level  


```{r, echo=T}
n=seq(10, 40, by=2) # vary sample size from 10 to 40
p0=0.5
k=qbinom(0.05, n, p0)-1 # significance level 0.05; p_0=0.5 
pa=0.3 # pa is any value within H_a=====
power=pbinom(k, n, pa) # calculate power when p=pa
ggplot(data=data.frame(n, k, power), aes(x=n, y=power, group=1)) +
  geom_line(color="red")+
  geom_hline(yintercept=0.8, linetype="dashed", color = "blue")+
  geom_point()+
  ylim(c(0,1))+
  ggtitle("p=0.3")

n=seq(10, 40, by=2) # vary sample size from 10 to 40
p0=0.5
k=qbinom(0.05, n, p0)-1 # significance level 0.05; p_0=0.5 
pa=0.2 # pa is any value within H_a
power=pbinom(k, n, pa) # calculate power when p=pa
ggplot(data=data.frame(n, k, power), aes(x=n, y=power, group=1)) +
  geom_line(color="red")+
  geom_hline(yintercept=0.8, linetype="dashed", color = "blue")+
  geom_point()+
  ylim(c(0,1))+
  ggtitle("p=0.2")
  
n=seq(10, 40, by=2) # vary sample size from 10 to 40 
k=qbinom(0.05, n, p0)-1 # significance level 0.05; p_0=0.5 
pa=0.1 # pa is any value within H_a
power=pbinom(k, n, pa) # calculate power when p=pa
ggplot(data=data.frame(n, k, power), aes(x=n, y=power, group=1)) +
  geom_line(color="red")+
  geom_hline(yintercept=0.8, linetype="dashed", color = "blue")+
  geom_point()+
  ylim(c(0,1))+
  ggtitle("p=0.1")


n=32
p0=0.5
k=qbinom(0.05, n, p0)-1 # significance level 0.05; p_0=0.5 
pa=seq(0, p0, by=0.1)
power=pbinom(k, n, pa) # calculate power when p=pa
ggplot(data=data.frame(n, k, power), aes(x=pa, y=power, group=1)) +
  geom_line(color="red")+
  geom_hline(yintercept=0.8, linetype="dashed", color = "blue")+
  geom_point()+
  ylim(c(0,1))
```

### two sample proportion test 

In two independent samples, $p_1, p_2$ are sample proportions. Consider 
 the test $H_0:p_1-p_2=p_0$ vs $H_a:p_1-p_2>p_0$, $p_0$ is the specified difference. Use test statistics $$Z=\frac{\hat{p_1}-\hat{p_2}-p_0}{\sqrt{\hat{p}(1-\hat{p}(1/n_1+1/n_2))}}$$, 
 
 where $\hat{p_1}=\frac{No.X=1}{n_1}, \hat{p_2}=\frac{No.Y=1}{n_2}$ are estimated proportions in two samples, and $\hat{p}=\frac{No.X=1+No.Y=1}{n_1+n_2}$ in the combined sample. Under null hypothesis, $Z\sim N(0.1)$ approximately. The test needs to be rejected when $Z>Z_{1-\alpha}$, e.g. $Z_{0.95}=1.64$. 
 

#### use pwr.2p.test

Use R function [pwr.2p.test](https://cran.r-project.org/web/packages/pwr/vignettes/pwr-vignette.html)

**G2 vs G3**

```{r, echo=T}
library(pwr)
pwr.2p.test(h = ES.h(p1 = 0.10, p2 = 0), sig.level = 0.05, power = .80, alternative ="greater") # failure rate in group 2 &3 are 10%, 0

pwr.2p.test(h = ES.h(p1 = 0.17, p2 = 0.05), sig.level = 0.05, power = .80,  alternative ="greater") # failure rate in group 2 &3 are 17%, 5%


pwr.2p.test(h = ES.h(p1 = 0.25, p2 = 0.10), sig.level = 0.05, power = .80,  alternative ="greater") # failure rate in group 2 &3 are 25%, 10%

```
```{r, echo=T}
library(pwr)
pwr.2p.test(h = ES.h(p1 = 0.10, p2 = 0), sig.level = 0.05, n=35, alternative ="greater") # failure rate in group 2 &3 are 10%, 0, power will be reported

pwr.2p.test(h = ES.h(p1 = 0.20, p2 = 0.10), sig.level = 0.05, power = .80,  alternative ="greater") # failure rate in group 2 &3 are 20%, 10%. Although same difference, the sample size needs to be increased a lot! 


```


